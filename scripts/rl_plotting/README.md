# Configure and start experiments

The following command quickly starts a list of experiments.
It runs a task graph with a given configuration three times.

```
source start_exp.sh
```

## Configuration

You can configure a list of experiments in the script.
The script runs applications under the whole combinations of the given configurations.

- APP_ARR: A list of applications to test.
           test_{APP_ARR[@]}.py scripts should be on the same directory.

- NUM_GPUS_ARR: A list of the number of GPUs to test.

- DATA_SIZE_ARR: A list of per data size in GB.

- BANDWIDTH_ARR: A list of bandwidth between GPU devices.
                 This and the application scripts assume a fully-connected
                 hardware topology.

- MAPPING_POLICIES: A list of online mapping policies to test.
                    The current simulator (05/05/2024) supports the following policies.
  - loadbalance: Assigns a task to the least committed device.
                 Note that it does not consider dependency and device available time,
                 but only planned/running task execution time accumulation for each device.
  - eft_without_data: Assigns a task to a device giving the earliest finish time.
                      Different from loadbalance, it considers task dependencies, and hence,
                      it estimates and exploits task's ready time and the earliest device
                      available time
  - eft_with_data: It is similar with eft_without_data, but it estimates data transfer time
                   on top of the task ready time.
  - random: It randomly chooses a device for a task.
  - heft: It is an online task mapping policy and replays task mapping deicisions created
          at the preprocessing time based on the HEFT offline scheduling policy.
 
- SORT_ARR: A list of task mapping orders to test.
            The current simulator (05/05/2024) supports the following sorting methods.
  - heft: Sorts the order by a HEFT rank.
  - random: Randomly sorts the order.
            (Note that the online HEFT policy also uses a random order if this mode is
             specified)

## Variability

### Task execution time noise

The current simulator supports and applies task execution time noise for both
compute and data movement tasks; the noise for the data movement task can be considered
as bandwidth or data size noise.

## Task mapping order

A task mapping order may affect task mapping quality.
We support "random" and "heft" orders. Please check the above `SORT_ARR` configuration.

## Outputs 

This script creates `[SORTING METHOD]_ouptuts` directory.
For example, if "heft" and "random" both are specified on `SORT_ARR`, 
both `heft_outputs` and `random_outputs` will be created.

A base name format for an output of each execution is
`[Application name]_[Number of GPUs]_[Per-data size in GB]_[Bandwidth in GB/s]`.
(For simplicity, we denote it by `fn`) 

This script creates multiple output files and directories.

- `fn_[iteration].out`: This includes all stdout logs.
- `fn_[iteration].noise`: This includes used task execution time noises
                          for each iteration (out of three repetitions).
- `fn_[iteration].order`: This includes used task mapping order for each
                          iteration (out of three repetitions).
- `fn_[iteration]_logs`: This directory includes each mapped task's start-end logs
                         for each GPU. It relies on the Python logger, and in logging.conf,
                         `logger_mapping` and `logger_launching` should be set to
                         the `DEBUG` level mode.
                         Depending on the policy, it has two types of logs.
  - `gpu[GPU ID].launching.log`: This includes actual start-end times for each task
                                 on each GPU.
  - `gpu[GPU ID].mapping.log`: This includes estimated start-end times for each task on each
                               GPU by the earliest-finish time-based mapping policies.
                               It is designed to compare the estimation and actual times.
- `fn.csv`: This includes each execution's total execution time in a csv format.
            It is used to generate plots. Note that this also includes theoretical bounds.
            Independent and serial task bounds are generated by
            either homog_theory.py and levelbylevel.py, and BSP (level-by-level) is generated within 
            the application scripts using networkx.
            NOTE that this relies on stdout prints redirected from the scripts.
            (e.g., `start_exp.sh` expects "[mapping policy],simtime,[exec time]" prints)
- `pdfs`: This directories include all plots generated by using `fn.csv`.
          `start_exp.sh` uses `simtime.R` to generate general execution time plots, and
          users can generate time breakdown plots (On 05/05/2024, this has a bug) by
          manually running `breakdown.sh`
          (This will execute `breakdown.R` to generate time plots).
  - `simtime.R`: This R script generates a plot. In this plot, labels for the bars are hard-coded to
                 ["Independent", "BSP", "HEFTTheory", "Serial", "heft", "loadbalance",
                  "eft_without_data", "eft_with_data", "random"]. If users modified the mapping 
                 policy configuration on `start_exp.sh`, this also should be changed properly.
                 Otherwise, this will generate incorrect plots or dummy bars.
    
# Convert plots in pdfs to pptx

We use scripts, `convert_to_pdf.sh` and `pdfs_to_ppts.py`, to convert from pdfs to pptx.
Users can execute `convert_to_pdf.sh` and it internally executes `pdfs_to_ppts.py`.

It creates 2 slides for 2 applications, each containing a 2x2 plot grid for 2 GPU counts and
2 bandwidths on top of the same per-data size.
NOTE that these parameters are hard-coded in `convert_to_pdf.sh`, and users should change them
properly. NOTE that pdf-formatted plots that users want to generate should be
on the `pdfs` direcotry.
