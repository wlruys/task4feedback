# Configure and start experiments

The following command quickly starts a set of experiments.
It runs a task graph with a given configuration three times.

```
source start_exp.sh
```

## Configuration

You can configure a set of experiments in the script.
The script runs applications under all combinations of the configurations.

- APP_ARR: A list of applications to test.
           test_{APP_ARR[@]}.py scripts should be on the same directory.

- NUM_GPUS_ARR: A list of the number of GPUs to test.

- DATA_SIZE_ARR: A list of per data size in GB.

- BANDWIDTH_ARR: A list of bandwidth between GPU devices.
                 This and the application scripts assume a fully-connected
                 hardware topology.

- MAPPING_POLICIES: A list of online mapping policies to test.
                    The current simulator (05/05/2024) supports the following policies.
  - loadbalance: Assigns a task to the least committed device.
                 Note that it does not consider dependency and device available time,
                 but only planned/running task execution time accumulation for each device.
  - eft_without_data: Assigns a task to a device giving the earliest finish time.
                      Different from loadbalance, it considers task dependencies, and hence,
                      it estimates and exploits a task's ready time and the earliest device
                      available time
  - eft_with_data: It is similar with eft_without_data, but it estimates data transfer time
                   on top of the task ready time.
  - random: It randomly chooses a device for a task.
  - heft: It replays task mapping deicisions created at the preprocessing time based on
          the HEFT offline scheduling policy at runtime.
 
- SORT_ARR: A list of task mapping order to test.
            The current simulator (05/05/2024) supports the following sorting methods.
  - heft: Sorts the order by a HEFT rank.
  - random: Randomly sorts the order.
            (Note that the online HEFT policy also uses a random order if this mode is
             specified)

## Variability

### Task execution time noise

The current simulator supports and applies task execution time noise for both
compute and data movement tasks; the noise for the data movement task is equal to
bandwidth or data size noise.

## Task mapping order

A task mapping order may affect task mapping quality.
We support "random" and "heft" orders. Please check the above `SORT_ARR` configuration.

## Outputs 

This script creates `[SORTING METHOD]_ouptuts` directory.
For example, if "heft" and "random" both are specified on `SORT_ARR`, 
both `heft_outputs` and `random_outputs` will be created.

A base name format for an output of each execution is
`[Application name]_[Number of GPUs]_[Per-data size in GB]_[Bandwidth in GB/s]`.
(For simplicity, we denote it by `fn`) 

This script creates multiple output files and directories.

- `fn_[iteration].out`: This includes all stdout logs.
- `fn_[iteration].noise`: This includes used task execution time noises
                          for each iteration (out of three repetitions).
- `fn_[iteration].order`: This includes used task mapping order for each
                          iteration (out of three repetitions).
- `fn_[iteration]_logs`: This directory includes each mapped task's start-end logs
                         for each GPU.
                         Depending on the policy, it has two types of logs.
  - `gpu[gpu ID].launching.log: This includes actual start-end times for each task
                                on each gpu.
  - `gpu[gpu ID].mapping.log: This is estimated start-end times for each task on each
                              gpu by earliest-finish time-based mapping policies.
                              It is designed to compare estimation and actual times.
- `fn.csv`: This includes each execution's total execution time in a csv format.
            It is used to generate plots.
            Note that this also includes theoretical bounds.
            Independent and serial task bounds are generated by
            either homog_theory.py and levelbylevel.py, and BSP is generated within 
            the application scripts using networkx.
            NOTE that this relies on stdout prints redirected from the scripts.
            (e.g., `start_exp.sh` expects "[mapping policy],simtime,[exec time]")
- `pdfs`: This directories include all plots generated by using `fn.csv`.
          `start_exp.sh` uses `simtime.R` to generate general execution time plots, and
          users can generate time breakdown plots (On 05/05/2024, this has a bug) by
          running `breakdown.sh` (This will execute `breakdown.R` to generate time plots).
    
# Convert plots in pdfs to pptx

We use scripts, `convert_to_pdf.sh` and `pdfs_to_ppts.py`, to convert from pdfs to pptx.
Users can execute `convert_to_pdf.sh` and then it internally executes `pdfs_to_ppts.py`.

It creates 2 slides for 2 applications, each containing a 2x2 plot grid for 2 gpu counts and
2 bandwidths on top of the same per-data size.
NOTE that these parameters are hard coded in `convert_to_pdf.sh`, and users should change them
properly. NOTE that pdf-formatted plots that users want to generate should be on the `pdfs` direcotry.
