defaults:
  - graph: dynamic_jacobi_diag
  - reward: incremental_schedule
  - system: 4h100
  - algorithm: ppo
  - runtime: batch
  - wandb: default
  - logging: default
  - lr_scheduler: none 
  - noise: none
  - optimizer: adamw
  - eval: default
  - _self_

hydra:
  callbacks:
    git_info:
      _target_: train.GitInfo

seed: 256
deterministic_torch: true

reward:
  verbose: False
  random_start: False
  gamma: 0.99
  extra_logging_policy: Oracle(32) # This is not what it is used for reward calculation. This is just for logging purpose.

graph:
  config:
    steps: 128
    n: 8
    level_memory: 40e9
    boundary_width: 0.25
  env:
    change_priority: False
    change_location: False
    change_duration: False
    seed: 1

algorithm:
  num_collections: 10000
  rollout_steps: 32
  workers: 4
  graphs_per_collection: 4
  ent_coef: 0.01
  sample_slices: False
  slice_len: 16
  gamma: 0.99
  lmbda: 0.95
  minibatch_size: 32
  collector: "multi_sync"

optimizer:
  lr: 1e-3

system:
  mem: 80e9

runtime:
  batch_size: 5
  queue_threshold: 5
  max_in_flight: 64

wandb:
  enabled: true
  project: 8x8x128_diag
  name: CNN-A_IEFT

eval:
  animation_interval: 0
  eval_interval: 25
  max_frames: 400
  video_seconds: 15
  samples: 1
  pickle_path: ./pickled_evaluation/8x8x128_diag_1:1:1.pkl

