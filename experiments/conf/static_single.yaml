defaults:
  - graph: static_jacobi
  - reward: pbrs
  - system: 4h100
  - feature: candidate_coordinate
  - network: vector_baseline
  - algorithm: ppo
  - runtime: batch
  - wandb: default
  - logging: default
  - lr_scheduler: none 
  - noise: none
  - optimizer: adamw
  - eval: default
  - _self_

hydra:
  callbacks:
    git_info:
      _target_: train.GitInfo
seed: 256
deterministic_torch: true

reward:
  verbose: False
  random_start: False
  gamma: 0.99
  scaling_factor: 1.0

graph:
  config:
    steps: 16
    level_memory: 80e9
  env:
    change_priority: False
    change_location: True
    change_duration: False
    seed: 1

algorithm:
  num_collections: 2000
  rollout_steps: 64
  workers: 4
  graphs_per_collection: 8
  ent_coef: 0.01
  sample_slices: False
  slice_len: 16 # only used if sample_slices is True (LSTM)
  gamma: 0.99
  lmbda: 0.95
  minibatch_size: 128
  collector: "multi_sync"

optimizer:
  lr: 5e-4

system:
  # mem: 99999e9
  mem: 80e9

feature:
  observer:
    batched: False

network:
  layers:
    # lstm:
    #   hidden_size: 64 # Adjust the LSTM hidden size by hidden_channels ** (log2(n)-1)
    state:
      hidden_channels: [64, 32, 16, 32]
    actor:
      hidden_channels: 32
    critic:
      hidden_channels: 32
  critic:
    add_progress: True

runtime:
  batch_size: 5
  queue_threshold: 5
  max_in_flight: 16

wandb:
  enabled: true
  project: static_4x4x16_ipdps
  name: MLP-A_IEFT

eval:
  eval_interval: 500
  max_frames: 400
  video_seconds: 15

